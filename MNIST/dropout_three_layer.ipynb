{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('num_iter', 20000, \"Number of training iterations\")\n",
    "tf.app.flags.DEFINE_integer('batch_size', 50, \"Batch size per iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DropNet():\n",
    "    def __init__(self, hps, images, labels, mode):\n",
    "        \"\"\"ResNet constructor.\n",
    "        Args:\n",
    "          hps: Hyperparameters.\n",
    "          images: Batches of images. [batch_size, image_size, image_size, 3]\n",
    "          labels: Batches of labels. [batch_size, num_classes]\n",
    "          mode: One of 'train' and 'eval'.\n",
    "        \"\"\"\n",
    "        self._images = images\n",
    "        self.labels = labels\n",
    "        self.mode = mode\n",
    "        self.hps = hps\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Build the core model within the graph.\"\"\"\n",
    "        with tf.variable_scope('init'):\n",
    "            x = self._images\n",
    "\n",
    "        with tf.variable_scope('fc'):\n",
    "            z = self.fully_connected(x, self.hps.kernel_shape, self.hps.bias_shape)\n",
    "        \n",
    "        with tf.variable_scope('logit'):\n",
    "            logits = self.logit(z, self.hps.num_classes)\n",
    "            self.predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        with tf.variable_scope('costs'):\n",
    "            xent = tf.nn.softmax_cross_entropy_with_logits(\n",
    "              logits=logits, labels=self.labels)\n",
    "            self.cost = tf.reduce_mean(xent, name='xent')\n",
    "            self.cost += self._decay()\n",
    "\n",
    "        tf.summary.scalar('cost', self.cost)\n",
    "        \n",
    "    def fully_connected(self, input, kernel_shape, bias_shape):\n",
    "        '''\n",
    "        one fully connected layer \n",
    "        input: input for the layer, kernel shape and bias shape\n",
    "        output: z of the layer, tf graph is constructed\n",
    "        '''\n",
    "        # Create variable named \"weights\".\n",
    "        weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "            initializer=tf.random_normal_initializer())\n",
    "        # Create variable named \"biases\".\n",
    "        biases = tf.get_variable(\"biases\", bias_shape,\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "        conv = tf.nn.conv2d(input, weights,\n",
    "            strides=[1, 1, 1, 1], padding='SAME')\n",
    "        return tf.nn.relu(tf.matmul(input, weights) + biases)\n",
    "\n",
    "        \n",
    "    def logit(self, x, out_dim):\n",
    "        \"\"\"FullyConnected layer for final output.\"\"\"\n",
    "        x = tf.reshape(x, [self.hps.batch_size, -1])\n",
    "        w = tf.get_variable(\n",
    "            'DW', [x.get_shape()[1], out_dim],\n",
    "            initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n",
    "        b = tf.get_variable('biases', [out_dim],\n",
    "                            initializer=tf.constant_initializer())\n",
    "        return tf.nn.xw_plus_b(x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_training_graph():\n",
    "    '''\n",
    "    1.\n",
    "    creates x, y placeholder\n",
    "    trains the graph with forward prop followed by backward prop\n",
    "    '''\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    logit = forward(x) # 1.1\n",
    "    loss = backward(logit, y) #1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(x, is_traning=True):\n",
    "    '''\n",
    "    1.1, 2.1\n",
    "    forward prop for training the neural network\n",
    "    NOW: Three layer fc NN.\n",
    "    '''\n",
    "    with tf.variable_scope(\"fc1\"):\n",
    "        # Variables created here will be named \"fc1/weights\", \"fc1/biases\".\n",
    "        relu1 = fc_layer(x, [5, 5, 32, 32], [32])\n",
    "    with tf.variable_scope(\"fc2\"):\n",
    "        # Variables created here will be named \"fc2/weights\", \"fc2/biases\".\n",
    "        return fc_layer(relu1, [5, 5, 32, 32], [32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(logit,y):\n",
    "    '''\n",
    "    1.2, 2.2\n",
    "    NOW: minimize softmax cross entropy\n",
    "    '''\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logit, y))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_eval_graph():\n",
    "    '''\n",
    "    2.\n",
    "    creates x, y placeholder\n",
    "    evaluates the graph with forward prop followed by backward prop\n",
    "    '''\n",
    "    x = tf.placeholder(tf.float32, [None, layer_sizes[0]])\n",
    "    y_ = tf.placeholder(tf.float32, [None, layer_sizes[-1]])\n",
    "    y_pred = forward(x, is_training=False)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y_,1))\n",
    "    loss = backward(logit,y)\n",
    "    accuracy = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer(input, kernel_shape, bias_shape):\n",
    "    '''\n",
    "    one fully connected layer \n",
    "    input: input for the layer, kernel shape and bias shape\n",
    "    output: z of the layer, tf graph is constructed\n",
    "    '''\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.random_normal_initializer())\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", bias_shape,\n",
    "        initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(tf.matmul(input, weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.variable_scope(\"MNIST_NN\") as scope:\n",
    "            build_training_graph() #1\n",
    "            scope.reuse_variables()\n",
    "            build_eval_graph() #2\n",
    "            \n",
    "        init = tf.initialize_all_variables()\n",
    "        sess = tf.Session()\n",
    "        \n",
    "    sess.run(init)\n",
    "    mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "    \n",
    "    for i in range(FLAGS.num_iter):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "              x:batch[0], y_: batch[1], is_train: False})\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], is_train: True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesresnet",
   "language": "python",
   "name": "bayesresnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
